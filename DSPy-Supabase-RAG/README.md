# DSPy Supabase RAG Pipeline

A production-ready RAG (Retrieval-Augmented Generation) pipeline with comprehensive evaluation.

## Tech Stack

| Component | Technology | Purpose |
|-----------|------------|---------|
| **PDF Parsing** | Docling | Layout analysis, table detection, OCR |
| **Embeddings** | sentence-transformers | all-MiniLM-L6-v2 (384 dims) |
| **Vector Store** | Supabase pgvector | HNSW index, hybrid search |
| **Keyword Search** | rank-bm25 | BM25Okapi algorithm |
| **LLM** | Groq | kimi-k2-instruct (fast inference) |
| **Framework** | DSPy | Structured LLM programming |
| **Evaluation** | RAGAS + LLM-as-Judge | Multi-metric assessment |

---

## How It Works

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           INGESTION PIPELINE                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

   ðŸ“„ PDF/DOCX/HTML                    
         â”‚                             
         â–¼                             
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    Docling extracts text, tables, 
   â”‚   Docling     â”‚    formulas with layout analysis.
   â”‚   Parser      â”‚    Falls back to OCR (EasyOCR) for
   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    scanned documents.
           â”‚                           
           â–¼                           
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    Splits into semantic chunks.
   â”‚   Chunking    â”‚    Optionally adds LLM-generated
   â”‚  + Context    â”‚    context prefixes (Anthropic's
   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    Contextual Retrieval technique).
           â”‚                           
           â–¼                           
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    sentence-transformers generates
   â”‚  Embeddings   â”‚    384-dim vectors locally.
   â”‚  (MiniLM)     â”‚    No API costs, runs on CPU/GPU.
   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                   
           â”‚                           
           â–¼                           
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    Stores vectors + metadata in
   â”‚   Supabase    â”‚    PostgreSQL with pgvector.
   â”‚   pgvector    â”‚    HNSW index for fast retrieval.
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            QUERY PIPELINE                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

   â“ User Question                    
         â”‚                             
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â–¼              â–¼              â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
   â”‚   BM25    â”‚  â”‚  Vector   â”‚        â”‚  HYBRID RETRIEVAL
   â”‚  Search   â”‚  â”‚  Search   â”‚        â”‚  
   â”‚ (keywords)â”‚  â”‚ (semantic)â”‚        â”‚  BM25 finds exact terms
   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜        â”‚  Vector finds meaning
         â”‚              â”‚              â”‚
         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
                â–¼                      â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
   â”‚  Reciprocal Rank    â”‚             â”‚  RRF combines rankings
   â”‚  Fusion (RRF)       â”‚             â”‚  without score normalization
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
              â”‚                        â”‚
              â–¼                        â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             
   â”‚  Top-K Documents    â”‚  Retrieved chunks with
   â”‚  + Metadata         â”‚  source, section, scores
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             
              â”‚                        
              â–¼                        
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  DSPy ChainOfThought
   â”‚   Groq LLM          â”‚  generates answer with
   â”‚   (kimi-k2)         â”‚  reasoning + sources
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             
              â”‚                        
              â–¼                        
   ðŸ’¬ Answer + Reasoning + Sources     


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          EVALUATION PIPELINE                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

   ðŸ“Š Test Set (questions + expected answers)
         â”‚
         â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚                   EVALUATION METHODS                     â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚                 â”‚                   â”‚                   â”‚
   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
   â”‚  â”‚  RAGAS    â”‚  â”‚  â”‚   DSPy      â”‚  â”‚  â”‚ LLM-as-     â”‚  â”‚
   â”‚  â”‚ (OpenAI)  â”‚  â”‚  â”‚ SemanticF1  â”‚  â”‚  â”‚   Judge     â”‚  â”‚
   â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â”‚
   â”‚        â”‚        â”‚         â”‚         â”‚         â”‚         â”‚
   â”‚  Industry       â”‚  Uses your       â”‚  Custom          â”‚
   â”‚  standard       â”‚  configured      â”‚  criteria,       â”‚
   â”‚  metrics        â”‚  LLM (Groq)      â”‚  no ground       â”‚
   â”‚                 â”‚                   â”‚  truth needed    â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                  â”‚                   â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚                    METRICS COMPUTED                      â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚      RETRIEVAL           â”‚        GENERATION            â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚  â€¢ Context Precision     â”‚  â€¢ Faithfulness              â”‚
   â”‚    (relevant docs?)      â”‚    (grounded in context?)    â”‚
   â”‚                          â”‚                              â”‚
   â”‚  â€¢ Context Recall        â”‚  â€¢ Answer Relevancy          â”‚
   â”‚    (all relevant found?) â”‚    (addresses question?)     â”‚
   â”‚                          â”‚                              â”‚
   â”‚  â€¢ Context Relevance     â”‚  â€¢ Answer Correctness        â”‚
   â”‚    (overall quality)     â”‚    (factually accurate?)     â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
   â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
   â•‘  Overall Score: 65.9%                                     â•‘
   â•‘  â”œâ”€ Retrieval:  49% (precision 44%, recall 54%)           â•‘
   â•‘  â””â”€ Generation: 83% (faithfulness 76%, relevancy 90%)     â•‘
   â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## Evaluation System

### Latest Results (20 test samples)

| Category | Metric | Score | Notes |
|----------|--------|-------|-------|
| **Overall** | Combined | **65.9%** | Good - minor optimization needed |
| **Retrieval** | Context Precision | 43.6% | Room for improvement |
| | Context Recall | 54.2% | Moderate coverage |
| **Generation** | Faithfulness | 75.6% | Good grounding |
| | Answer Relevancy | 90.1% | Excellent |

*Evaluated with RAGAS using gpt-4o-mini as the judge model. RAG queries powered by Groq kimi-k2-instruct.*

### Three Evaluation Approaches

| Approach | Judge Model | Best For | Speed |
|----------|-------------|----------|-------|
| **RAGAS** | gpt-4o-mini (OpenAI) | Production benchmarks | ~2 min |
| **DSPy SemanticF1** | Your LLM (Groq/Gemini) | Answer correctness | Medium |
| **LLM-as-Judge** | Your LLM (Groq/Gemini) | Quick checks, no ground truth | Fast |

### Evaluation Metrics Explained

#### Retrieval Metrics

| Metric | What It Measures | How It's Computed |
|--------|------------------|-------------------|
| **Context Precision** | Are retrieved docs relevant? | LLM judges each chunk's relevance to query |
| **Context Recall** | Did we get all relevant info? | Compares retrieved vs. ground truth claims |
| **Context Relevance** | Overall retrieval quality | Combined precision/recall score |

#### Generation Metrics

| Metric | What It Measures | How It's Computed |
|--------|------------------|-------------------|
| **Faithfulness** | Is answer grounded in context? | Extracts claims, verifies each against context |
| **Answer Relevancy** | Does answer address the question? | Generates reverse questions, measures similarity |
| **Answer Correctness** | Is answer factually correct? | SemanticF1 vs. ground truth |

### Running Evaluation

Evaluation uses `save_questions_to_faq=False` so test queries do not pollute your FAQ store.

```python
from evaluation import PipelineEvaluator
from rag_pipeline import RAGSystem

# Initialize
rag = RAGSystem()
evaluator = PipelineEvaluator(rag)

# Quick eval (no ground truth needed)
result = evaluator.quick_eval([
    "What is the main topic?",
    "What are the key findings?",
])
print(result)

# Full eval with ground truth
result = evaluator.full_eval([
    {"question": "What is X?", "expected_answer": "X is..."},
])
evaluator.generate_report(result, "eval_results.json")
```

### Interpreting Results

| Score | Interpretation | Action |
|-------|----------------|--------|
| **> 80%** | Excellent | Production ready |
| **60-80%** | Good | Minor optimization |
| **40-60%** | Fair | Review retrieval/prompts |
| **< 40%** | Poor | Major debugging needed |

### Sample-Level Insights

Results include per-question scores, helping identify weak spots:

```json
{
  "question": "How does Docling handle tables?",
  "faithfulness": 1.0,        // âœ“ Perfect - answer grounded in context
  "answer_relevancy": 0.90,   // âœ“ Excellent - addresses the question
  "context_precision": 0.53,  // â–³ Moderate - some irrelevant chunks retrieved
  "context_recall": 1.0       // âœ“ Perfect - all relevant info found
}
```

**Common patterns:**
- High faithfulness + low recall â†’ Retrieval missing relevant docs
- Low faithfulness + high recall â†’ Generation hallucinating despite good context
- Low relevancy â†’ Question-answer mismatch, check prompt

---

## Quick Start

> **ðŸ“– See [START_HERE.md](START_HERE.md) for detailed step-by-step instructions**

```bash
# 1. Install (using uv - 10x faster than pip)
uv venv && source .venv/bin/activate
uv pip install -r requirements.txt

# 2. Configure
cp .env.example .env  # Add your API keys

# 3. Setup Supabase (run SQL from START_HERE.md)

# 4. Download sample PDFs
uv run download_samples.py

# 5. Ingest & Query
uv run rag_pipeline.py ingest sample_pdfs/*.pdf
uv run rag_pipeline.py interactive
```

---

## Usage

### Python API

```python
from rag_pipeline import RAGSystem

# Initialize
rag = RAGSystem(
    llm_provider="groq",
    llm_model="moonshotai/kimi-k2-instruct",
    hybrid_retrieval=True,
    save_questions_to_faq=True,  # set False to skip FAQ logging
)

# Ingest documents
rag.ingest("document.pdf")

# Query
response = rag.query("What are the key findings?")
print(response.answer)
print(response.reasoning)
print(response.sources)
```

### CLI Commands

```bash
# Ingest
uv run rag_pipeline.py ingest document.pdf
uv run rag_pipeline.py ingest *.pdf

# Query
uv run rag_pipeline.py query "Your question here"
uv run rag_pipeline.py interactive
uv run rag_pipeline.py query "Your question here" --no-save-faq  # skip FAQ logging
uv run rag_pipeline.py interactive --no-save-faq  # interactive without logging

# Evaluate
uv run evaluation.py quick -q "Question 1" "Question 2"
uv run evaluation.py full -f test_set.json -o results.json

# Slower rate for free-tier APIs (default 3s delay)
uv run evaluation.py full -f test_set.json -o results.json --delay 5
```

---

## Configuration

### Required Environment Variables

```env
SUPABASE_URL=https://xxx.supabase.co
SUPABASE_KEY=your-service-role-key
GROQ_API_KEY=gsk_xxx
```

### Optional Environment Variables

```env
OPENAI_API_KEY=sk-xxx        # For RAGAS evaluation
GEMINI_API_KEY=xxx           # Alternative LLM
```

### Model Options

| Provider | Model | Use Case |
|----------|-------|----------|
| **Groq** | `moonshotai/kimi-k2-instruct` | Default - powerful reasoning |
| **Groq** | `llama-3.3-70b-versatile` | Fast, general purpose |
| **Gemini** | `gemini-2.5-flash` | Alternative provider |

---

## Project Structure

```
DSPy-Supabase-RAG/
â”œâ”€â”€ rag_pipeline.py       # Main RAG system
â”œâ”€â”€ pdf_processor.py      # Docling PDF parsing
â”œâ”€â”€ embeddings.py         # Embedding generation + Supabase
â”œâ”€â”€ retriever.py          # Hybrid search (BM25 + vector)
â”œâ”€â”€ evaluation.py         # RAGAS + LLM-as-Judge
â”œâ”€â”€ download_samples.py   # Download test PDFs
â”œâ”€â”€ requirements.txt      # Dependencies
â”œâ”€â”€ pyproject.toml        # Modern Python config
â”œâ”€â”€ .env.example          # Environment template
â”œâ”€â”€ START_HERE.md         # Quick start guide
â””â”€â”€ README.md             # This file
```

---

## Features

### Docling PDF Processing
- Layout analysis and table detection
- OCR fallback (EasyOCR/Tesseract)
- Formula and image extraction
- Multiple chunking strategies (semantic/fixed/paragraph)

### Hybrid Retrieval
- **BM25**: Exact keyword matching
- **Vector**: Semantic similarity (cosine distance)
- **RRF**: Reciprocal Rank Fusion combines both

### Contextual Chunking
Based on [Anthropic's Contextual Retrieval](https://www.anthropic.com/news/contextual-retrieval) (reduces failures by 67%):

```
Before: "Revenue grew by 15%..."
After:  "[This chunk is from ACME's Q3 2024 report, discussing quarterly revenue growth.]
         Revenue grew by 15%..."
```

### FAQ Capture (Question Logging)
- Each answered question is stored back into Supabase as a FAQ chunk (metadata `type=faq`), so proven answers become retrievable context for future queries.
- Hybrid retrieval picks them up automatically; disable when you do not want this behavior with `RAGSystem(save_questions_to_faq=False)` or CLI `--no-save-faq`.

### Multi-Strategy Evaluation
- **RAGAS**: Industry-standard metrics (requires OpenAI)
- **SemanticF1**: DSPy's fact-based comparison
- **LLM-as-Judge**: Custom criteria, explainable scores

---

## Troubleshooting

| Error | Solution |
|-------|----------|
| `SUPABASE_URL not found` | Copy `.env.example` to `.env`, add credentials |
| `match_documents not found` | Run SQL setup in Supabase (see START_HERE.md) |
| `OcrOptions error` | Update to latest Docling: `pip install -U docling` |
| Slow embeddings | Use GPU: `EmbeddingGenerator(device="cuda")` |

---

## License

MIT
